{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Dataset api\n",
    "\n",
    "Try to use the new TF dataset api to setup the input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from dataset import dataset_common\n",
    "from preprocessing import ssd_preprocessing\n",
    "from utility import anchor_manipulator\n",
    "from utility import scaffolds\n",
    "\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOC_LABELS = {\n",
    "    'none': (0, 'Background'),\n",
    "    'aeroplane': (1, 'Vehicle'),\n",
    "    'bicycle': (2, 'Vehicle'),\n",
    "    'bird': (3, 'Animal'),\n",
    "    'boat': (4, 'Vehicle'),\n",
    "    'bottle': (5, 'Indoor'),\n",
    "    'bus': (6, 'Vehicle'),\n",
    "    'car': (7, 'Vehicle'),\n",
    "    'cat': (8, 'Animal'),\n",
    "    'chair': (9, 'Indoor'),\n",
    "    'cow': (10, 'Animal'),\n",
    "    'diningtable': (11, 'Indoor'),\n",
    "    'dog': (12, 'Animal'),\n",
    "    'horse': (13, 'Animal'),\n",
    "    'motorbike': (14, 'Vehicle'),\n",
    "    'person': (15, 'Person'),\n",
    "    'pottedplant': (16, 'Indoor'),\n",
    "    'sheep': (17, 'Animal'),\n",
    "    'sofa': (18, 'Indoor'),\n",
    "    'train': (19, 'Vehicle'),\n",
    "    'tvmonitor': (20, 'Indoor'),\n",
    "}\n",
    "# TODO: generate the training shards path automatically\n",
    "training_files = [\"/playground/data/VOCdevkit/tfdb/train-00000-of-00004\", \"/playground/data/VOCdevkit/tfdb/train-00001-of-00004\",\n",
    "                 \"/playground/data/VOCdevkit/tfdb/train-00002-of-00004\", \"/playground/data/VOCdevkit/tfdb/train-00003-of-00004\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_mapping_fn(example_proto, is_training, image_preprocessing_fn, anchor_encoder):\n",
    "    keys_to_features = {\n",
    "        'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\n",
    "        'image/format': tf.FixedLenFeature((), tf.string, default_value='jpeg'),\n",
    "        'image/filename': tf.FixedLenFeature((), tf.string, default_value=''),\n",
    "        'image/height': tf.FixedLenFeature([1], tf.int64),\n",
    "        'image/width': tf.FixedLenFeature([1], tf.int64),\n",
    "        'image/channels': tf.FixedLenFeature([1], tf.int64),\n",
    "        'image/shape': tf.FixedLenFeature([3], tf.int64),\n",
    "        'image/object/bbox/xmin': tf.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox/ymin': tf.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox/xmax': tf.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox/ymax': tf.VarLenFeature(dtype=tf.float32),\n",
    "        'image/object/bbox/label': tf.VarLenFeature(dtype=tf.int64),\n",
    "        'image/object/bbox/difficult': tf.VarLenFeature(dtype=tf.int64),\n",
    "        'image/object/bbox/truncated': tf.VarLenFeature(dtype=tf.int64),\n",
    "    }\n",
    "    items_to_handlers = {\n",
    "        'image': slim.tfexample_decoder.Image('image/encoded', 'image/format'),\n",
    "        'filename': slim.tfexample_decoder.Tensor('image/filename'),\n",
    "        'shape': slim.tfexample_decoder.Tensor('image/shape'),\n",
    "        'object/bbox': slim.tfexample_decoder.BoundingBox(\n",
    "                ['ymin', 'xmin', 'ymax', 'xmax'], 'image/object/bbox/'),\n",
    "        'object/label': slim.tfexample_decoder.Tensor('image/object/bbox/label'),\n",
    "        'object/difficult': slim.tfexample_decoder.Tensor('image/object/bbox/difficult'),\n",
    "        'object/truncated': slim.tfexample_decoder.Tensor('image/object/bbox/truncated'),\n",
    "    }\n",
    "    decoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features, items_to_handlers)\n",
    "    [org_image, filename, shape, glabels_raw, gbboxes_raw, isdifficult] = decoder.decode(example_proto, ['image', 'filename', 'shape',\n",
    "                                                                     'object/label',\n",
    "                                                                     'object/bbox',\n",
    "                                                                     'object/difficult'])\n",
    "    \n",
    "    if is_training:\n",
    "        # if all is difficult, then keep the first one\n",
    "        isdifficult_mask =tf.cond(tf.count_nonzero(isdifficult, dtype=tf.int32) < tf.shape(isdifficult)[0],\n",
    "                                lambda : isdifficult < tf.ones_like(isdifficult),\n",
    "                                lambda : tf.one_hot(0, tf.shape(isdifficult)[0], on_value=True, off_value=False, dtype=tf.bool))\n",
    "\n",
    "        glabels_raw = tf.boolean_mask(glabels_raw, isdifficult_mask)\n",
    "        gbboxes_raw = tf.boolean_mask(gbboxes_raw, isdifficult_mask)\n",
    "\n",
    "    # Pre-processing image, labels and bboxes.\n",
    "\n",
    "    if is_training:\n",
    "        image, glabels, gbboxes = image_preprocessing_fn(org_image, glabels_raw, gbboxes_raw)\n",
    "    else:\n",
    "        image = image_preprocessing_fn(org_image, glabels_raw, gbboxes_raw)\n",
    "        glabels, gbboxes = glabels_raw, gbboxes_raw\n",
    "\n",
    "    gt_targets, gt_labels, gt_scores = anchor_encoder(glabels, gbboxes)\n",
    "    \n",
    "    # return [image, filename, shape, gt_targets, gt_labels, gt_scores]\n",
    "    return image, {'shape': shape, 'loc_targets': gt_targets, 'cls_targets': gt_labels, 'match_scores': gt_scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FLAGS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-8f7f22482812>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0minput_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train-*'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mout_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_image_size\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         anchor_creator = anchor_manipulator.AnchorCreator(out_shape,\n\u001b[1;32m      5\u001b[0m                                                     \u001b[0mlayers_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m38\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m38\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m19\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m19\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FLAGS' is not defined"
     ]
    }
   ],
   "source": [
    "def input_pipeline(training_files='train-*', is_training=True, batch_size=FLAGS.batch_size):\n",
    "    def input_fn():\n",
    "        out_shape = [FLAGS.train_image_size] * 2\n",
    "        anchor_creator = anchor_manipulator.AnchorCreator(out_shape,\n",
    "                                                    layers_shapes = [(38, 38), (19, 19), (10, 10), (5, 5), (3, 3), (1, 1)],\n",
    "                                                    anchor_scales = [(0.1,), (0.2,), (0.375,), (0.55,), (0.725,), (0.9,)],\n",
    "                                                    extra_anchor_scales = [(0.1414,), (0.2739,), (0.4541,), (0.6315,), (0.8078,), (0.9836,)],\n",
    "                                                    anchor_ratios = [(1., 2., .5), (1., 2., 3., .5, 0.3333), (1., 2., 3., .5, 0.3333), (1., 2., 3., .5, 0.3333), (1., 2., .5), (1., 2., .5)],\n",
    "                                                    layer_steps = [8, 16, 32, 64, 100, 300])\n",
    "        all_anchors, all_num_anchors_depth, all_num_anchors_spatial = anchor_creator.get_all_anchors()\n",
    "\n",
    "        num_anchors_per_layer = []\n",
    "        for ind in range(len(all_anchors)):\n",
    "            num_anchors_per_layer.append(all_num_anchors_depth[ind] * all_num_anchors_spatial[ind])\n",
    "\n",
    "        anchor_encoder_decoder = anchor_manipulator.AnchorEncoder(allowed_borders = [1.0] * 6,\n",
    "                                                            positive_threshold = FLAGS.match_threshold,\n",
    "                                                            ignore_threshold = FLAGS.neg_threshold,\n",
    "                                                            prior_scaling=[0.1, 0.1, 0.2, 0.2])\n",
    "\n",
    "        image_preprocessing_fn = lambda image_, labels_, bboxes_ : ssd_preprocessing.preprocess_image(image_, labels_, bboxes_, out_shape, is_training=is_training, data_format=FLAGS.data_format, output_rgb=False)\n",
    "        anchor_encoder_fn = lambda glabels_, gbboxes_: anchor_encoder_decoder.encode_all_anchors(glabels_, gbboxes_, all_anchors, all_num_anchors_depth, all_num_anchors_spatial)\n",
    "\n",
    "        filenames = tf.placeholder(tf.string, shape=[None])\n",
    "        arga = tf.constant(True)\n",
    "\n",
    "        dataset = tf.data.TFRecordDataset(training_files)\n",
    "        dataset = dataset.map(lambda x: data_mapping_fn(x, is_training, image_preprocessing_fn, anchor_encoder))\n",
    "        dataset = dataset.repeat() # repeat the input infinitely\n",
    "        dataset = dataset.batch(batch_size) # set the batch size\n",
    "        iterator = dataset.make_initializable_iterator()\n",
    "        \n",
    "#         image, _, shape, loc_targets, cls_targets, match_scores = dataset_common.slim_get_batch(FLAGS.num_classes,\n",
    "#                                                                                 batch_size,\n",
    "#                                                                                 ('train' if is_training else 'val'),\n",
    "#                                                                                 os.path.join(FLAGS.data_dir, dataset_pattern),\n",
    "#                                                                                 FLAGS.num_readers,\n",
    "#                                                                                 FLAGS.num_preprocessing_threads,\n",
    "#                                                                                 image_preprocessing_fn,\n",
    "#                                                                                 anchor_encoder_fn,\n",
    "#                                                                                 num_epochs=FLAGS.train_epochs,\n",
    "#                                                                                 is_training=is_training)\n",
    "        global global_anchor_info\n",
    "        global_anchor_info = {'decode_fn': lambda pred : anchor_encoder_decoder.decode_all_anchors(pred, num_anchors_per_layer),\n",
    "                            'num_anchors_per_layer': num_anchors_per_layer,\n",
    "                            'all_num_anchors_depth': all_num_anchors_depth }\n",
    "\n",
    "        # return image, {'shape': shape, 'loc_targets': loc_targets, 'cls_targets': cls_targets, 'match_scores': match_scores}\n",
    "        return dataset\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = tf.placeholder(tf.string, shape=[None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "arga = tf.constant(True)\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "dataset = dataset.map(lambda x: data_mapping_fn(x, is_training))\n",
    "dataset = dataset.repeat() # repeat the input infinitely\n",
    "dataset = dataset.batch(32) # set the batch size\n",
    "iterator = dataset.make_initializable_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Cannot batch tensors with different shapes in component 0. First element had shape [500,307,3] and element 1 had shape [333,500,3].\n\t [[Node: IteratorGetNext_6 = IteratorGetNext[output_shapes=[[?,?,?,3], [?], [?,3], [?,?], [?,?,4], [?,?]], output_types=[DT_UINT8, DT_STRING, DT_INT64, DT_INT64, DT_FLOAT, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator_8)]]\n\nCaused by op u'IteratorGetNext_6', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 1064, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2714, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2818, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2878, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-73-3e045f10ff77>\", line 7, in <module>\n    print sess.run(iterator.get_next())\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 370, in get_next\n    name=name)), self._output_types,\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1466, in iterator_get_next\n    output_shapes=output_shapes, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Cannot batch tensors with different shapes in component 0. First element had shape [500,307,3] and element 1 had shape [333,500,3].\n\t [[Node: IteratorGetNext_6 = IteratorGetNext[output_shapes=[[?,?,?,3], [?], [?,3], [?,?], [?,?,4], [?,?]], output_types=[DT_UINT8, DT_STRING, DT_INT64, DT_INT64, DT_FLOAT, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator_8)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-3e045f10ff77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# print iterator.output_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# print iterator.output_shapes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Cannot batch tensors with different shapes in component 0. First element had shape [500,307,3] and element 1 had shape [333,500,3].\n\t [[Node: IteratorGetNext_6 = IteratorGetNext[output_shapes=[[?,?,?,3], [?], [?,3], [?,?], [?,?,4], [?,?]], output_types=[DT_UINT8, DT_STRING, DT_INT64, DT_INT64, DT_FLOAT, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator_8)]]\n\nCaused by op u'IteratorGetNext_6', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 1064, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2714, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2818, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2878, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-73-3e045f10ff77>\", line 7, in <module>\n    print sess.run(iterator.get_next())\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 370, in get_next\n    name=name)), self._output_types,\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1466, in iterator_get_next\n    output_shapes=output_shapes, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Cannot batch tensors with different shapes in component 0. First element had shape [500,307,3] and element 1 had shape [333,500,3].\n\t [[Node: IteratorGetNext_6 = IteratorGetNext[output_shapes=[[?,?,?,3], [?], [?,3], [?,?], [?,?,4], [?,?]], output_types=[DT_UINT8, DT_STRING, DT_INT64, DT_INT64, DT_FLOAT, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator_8)]]\n"
     ]
    }
   ],
   "source": [
    "# init iterator with training data\n",
    "sess = tf.Session()\n",
    "sess.run(iterator.initializer, feed_dict={filenames: training_files, is_training: True})\n",
    "\n",
    "# print iterator.output_classes\n",
    "# print iterator.output_shapes\n",
    "print sess.run(iterator.get_next())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
